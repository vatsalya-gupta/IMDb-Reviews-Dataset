{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "imdb_reviews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hb15qJcekavI"
      },
      "source": [
        "#**Text Classification with the IMDb-Reviews Dataset from Keras**\n",
        "@author: [vatsalya-gupta](https://github.com/vatsalya-gupta)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z6EBWDDYEtSx",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "imdb = keras.datasets.imdb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7kOEthtfnkBu"
      },
      "source": [
        "We will do our analysis with the 88000 most frequent unique words in our dataset. Here, we are making the train-test split, 80 % and 20 % respectively. Afterwards, we will split train_data into training and validation sets, making the final train-test-validate split be 70-20-10 %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iwPp1ourJAN5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "35bc69d0-9782-42d0-870c-4fa7ea1b7793"
      },
      "source": [
        "(train_data, train_targets), (test_data, test_targets) = imdb.load_data(num_words = 88000)\n",
        "data = np.concatenate((test_data, train_data), axis=0)\n",
        "targets = np.concatenate((test_targets, train_targets), axis=0)\n",
        "\n",
        "test_data = data[:10000]\n",
        "test_labels = targets[:10000]\n",
        "train_data = data[10000:]\n",
        "train_labels = targets[10000:]\n",
        "\n",
        "print(train_data[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 670, 5304, 5622, 13500, 308, 8551, 23033, 25, 71, 1017, 6, 253, 22, 4, 436, 223, 100, 358, 134, 5, 85, 907, 71, 540, 2218, 88, 36, 28, 24, 1477, 4, 2483, 21, 1075, 12, 18, 148, 15, 3824, 15, 36, 122, 24, 34114, 8, 4, 204, 65, 7, 4, 1422, 89, 400, 127, 4, 228, 11, 6, 22, 2555, 2198, 8, 51, 9, 170, 23, 11, 4, 22, 12, 9, 4, 1310, 15, 5442, 14, 9, 51, 13, 264, 4, 907, 7, 134, 102, 71, 399, 1855, 6, 2392, 1310, 18, 154, 9485, 18, 4, 91, 173, 36, 3115, 1669, 19, 32, 134, 9485, 37, 9, 170, 8, 40, 98, 32, 75, 100, 28, 343, 53, 5355, 10, 10, 417, 51, 9, 498, 60, 1422, 209, 6, 171, 1298, 372]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qu7omIyTlMq7"
      },
      "source": [
        "Our training and testing data is in the form of an array of reviews, where each review is a list of integers and each integer represents a unique word. So we need to make it human readable. For this, we will be adding the following tags to the data, map the values to their respective keys and implement a function which converts the integers to the respective words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oLm7rhdcJZPD",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2    # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "np.save(\"word_index.npy\", word_index)    # saving the word_index for future use"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a9Ck0V79ledx",
        "colab": {}
      },
      "source": [
        "def decode_review(text):\n",
        "\treturn \" \".join([reverse_word_index.get(i, \"?\") for i in text])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TpL5VpjHL02l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "341e9e74-2590-4004-dcc2-121cca442094"
      },
      "source": [
        "print(decode_review(train_data[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> robert altman nicolas roeg john luc goddard you were expecting a fun film the entire family could enjoy these and other directors were obviously chosen because they have not followed the mainstream but created it for those that complain that they did not adhere to the original story of the opera how often does the music in a film directly relate to what is going on in the film it is the mood that counts this is what i believe the directors of these movies were doing creating a contemporary mood for old operas for the most part they succeed wonderfully with all these operas who is going to like them all we could have used more beverly br br finally what is art even opera without a few naked women\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AaChBIYGM3jI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56fd9520-d8e5-4501-e8c5-c298c82b7029"
      },
      "source": [
        "print(len(train_data[0]), len(test_data[0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132 68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DuIMtysMndkN"
      },
      "source": [
        "In the following block of code, we will be finding the length of the longest review in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q2fFYnOfNGQr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b26e96a0-7f61-4f50-acff-638423cb2ce7"
      },
      "source": [
        "longest_train = max(len(l) for l in train_data)\n",
        "longest_test = max(len(l) for l in test_data)\n",
        "\n",
        "max_words = max(longest_train, longest_test)\n",
        "print(max_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6zlusI0RoQ5a"
      },
      "source": [
        "Even though the longest review is 2494 words long, we can safely limit the length of our reviews to 500 words as most of them are well below that. For the ones with length less than 500 words, we will add zero padding to their end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mna5HgcdQLgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "941c14de-0bca-4ed9-c421-1d5d5adee728"
      },
      "source": [
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 500)\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 500)\n",
        "\n",
        "print(len(train_data[0]), len(test_data[0]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ucvkCeKrqiJx"
      },
      "source": [
        "We are using a Sequential model. An Embedding layer attempts to determine the meaning of each word in the sentence by mapping each word to a position in vector space (helps in grouping words like \"fantastic\" and \"awesome\"). The GlobalAveragePooling1D layer scales down our data's dimensions to make it easier computationally. The last two layers in our network are dense fully connected layers. The output layer is one neuron that uses the sigmoid function to get a value between 0 and 1 which will represent the likelihood of the review being negative or positive respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NL8OsQrRQ94w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "575b2f1b-693e-403e-ffeb-2eae6806bb29"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(88000, 16))    # 88000 words as input\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          1408000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,408,289\n",
            "Trainable params: 1,408,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8CZdpVPYsRTH"
      },
      "source": [
        "Compiling the data using the following parameters. We are using loss as \"binary_crossentropy\", as the expected output of our model is either 0 or 1, that is negative or positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1HLinov6UMcA",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gFZ8IzeYs7m0"
      },
      "source": [
        "Here we split the training data into training and validation sets, then the training data is fit onto the model and the results are evaluated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wrvl9V3aSx1o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5e007f2d-3581-48d8-8123-1e91cac28119"
      },
      "source": [
        "model.fit(train_data, train_labels, epochs = 10, batch_size = 256, validation_split = 0.125, verbose = 1)\n",
        "model.evaluate(test_data, test_labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.6888 - accuracy: 0.6169 - val_loss: 0.6766 - val_accuracy: 0.7644\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 3s 21ms/step - loss: 0.6332 - accuracy: 0.7807 - val_loss: 0.5698 - val_accuracy: 0.8062\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 3s 23ms/step - loss: 0.4974 - accuracy: 0.8387 - val_loss: 0.4368 - val_accuracy: 0.8548\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 3s 23ms/step - loss: 0.3755 - accuracy: 0.8752 - val_loss: 0.3574 - val_accuracy: 0.8724\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.3021 - accuracy: 0.8969 - val_loss: 0.3174 - val_accuracy: 0.8836\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 3s 23ms/step - loss: 0.2575 - accuracy: 0.9113 - val_loss: 0.2954 - val_accuracy: 0.8880\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.2262 - accuracy: 0.9224 - val_loss: 0.2818 - val_accuracy: 0.8932\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 3s 23ms/step - loss: 0.2009 - accuracy: 0.9309 - val_loss: 0.2747 - val_accuracy: 0.8950\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.1812 - accuracy: 0.9387 - val_loss: 0.2725 - val_accuracy: 0.8942\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.1637 - accuracy: 0.9452 - val_loss: 0.2643 - val_accuracy: 0.9020\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2526 - accuracy: 0.9030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2526327669620514, 0.902999997138977]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zXGei7y3pU_2"
      },
      "source": [
        "Sample prediction from testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NXDGSeVGYod8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3c777520-773a-49b5-e0ec-4bb76de3826d"
      },
      "source": [
        "test_review = test_data[0]\n",
        "predict = model.predict(test_review)\n",
        "print(\"Review:\\n\", decode_review(test_review))\n",
        "print(\"Prediction:\", predict[0])\n",
        "print(\"Actual:\", test_labels[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review:\n",
            " <START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "Prediction: [5.7836072e-08]\n",
            "Actual: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QloUm6_ppYQf"
      },
      "source": [
        "Saving the model so that we don't have to train it again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tSR71k3jbKji",
        "colab": {}
      },
      "source": [
        "model.save(\"imdb_model.h5\")    # any name ending with .h5\n",
        "# model = keras.models.load_model(\"imdb_model.h5\")    # loading the model, use this in any other project for testing"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iLkyZ73npn-A"
      },
      "source": [
        "Function to encode a text based review into a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IShn516EiWP-",
        "colab": {}
      },
      "source": [
        "def review_encode(s):\n",
        "\tencoded = [1]    # 1 implies \"<START>\"\n",
        "\n",
        "\tfor word in s:\n",
        "\t\tif word.lower() in word_index:\n",
        "\t\t\tencoded.append(word_index[word.lower()] if (word_index[word.lower()] < 88001) else 2)    # vocabulary size is 88000\n",
        "\t\telse:\n",
        "\t\t\tencoded.append(2)    # 2 implies \"<UNK>\"\n",
        "\n",
        "\treturn encoded"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xoldbSdrpvuR"
      },
      "source": [
        "Evaluating our model on an [external review](https://www.imdb.com/review/rw2284594)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FxVejEqpiLw0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "04789103-426f-42e7-cb36-826c548773bb"
      },
      "source": [
        "with open(\"sample_data/test.txt\", encoding = \"utf-8\") as f:\n",
        "\tfor line in f.readlines():\n",
        "\t\tnline = line.replace(\",\", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace(\"\\\"\",\"\").strip().split(\" \")\n",
        "\t\tencode = review_encode(nline)\n",
        "\t\tencode = keras.preprocessing.sequence.pad_sequences([encode], value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 500)    # make the review 500 words long\n",
        "\t\tpredict = model.predict(encode)\n",
        "\t\tprint(line, \"\\n\", encode, \"\\n\", predict[0])\n",
        "\t\tsentiment = \"Positive\" if (predict[0] > 0.5) else \"Negative\"\n",
        "\t\tprint(\"Sentiment:\", sentiment)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Shawshank Redemption is written and directed by Frank Darabont. It is an adaptation of the Stephen King novella Rita Hayworth and Shawshank Redemption. Starring Tim Robbins and Morgan Freeman, the film portrays the story of Andy Dufresne (Robbins), a banker who is sentenced to two life sentences at Shawshank State Prison for apparently murdering his wife and her lover. Andy finds it tough going but finds solace in the friendship he forms with fellow inmate Ellis \"Red\" Redding (Freeman). While things start to pick up when the warden finds Andy a prison job more befitting his talents as a banker. However, the arrival of another inmate is going to vastly change things for all of them. There was no fanfare or bunting put out for the release of the film back in 94, with a title that didn't give much inkling to anyone about what it was about, and with Columbia Pictures unsure how to market it, Shawshank Redemption barely registered at the box office. However, come Academy Award time the film received several nominations, and although it won none, it stirred up interest in the film for its home entertainment release. The rest, as they say, is history. For the film finally found an audience that saw the film propelled to almost mythical proportions as an endearing modern day classic. Something that has delighted its fans, whilst simultaneously baffling its detractors. One thing is for sure, though, is that which ever side of the Shawshank fence you sit on, the film continues to gather new fans and simply will never go away or loose that mythical status. It's possibly the simplicity of it all that sends some haters of the film into cinematic spasms. The implausible plot and an apparent sentimental edge that makes a nonsense of prison life, are but two chief complaints from those that dislike the film with a passion. Yet when characters are this richly drawn, and so movingly performed, it strikes me as churlish to do down a human drama that's dealing in hope, friendship and faith. The sentimental aspect is indeed there, but that acts as a counterpoint to the suffering, degradation and shattering of the soul involving our protagonist. Cosy prison life you say? No chance. The need for human connection is never more needed than during incarceration, surely? And given the quite terrific performances of Robbins (never better) & Freeman (sublimely making it easy), it's the easiest thing in the world to warm to Andy and Red. Those in support aren't faring too bad either. Bob Gunton is coiled spring smarm as Warden Norton, James Whitmore is heart achingly great as the \"Birdman Of Shawshank,\" Clancy Brown is menacing as antagonist Capt. Byron Hadley, William Sadler amusing as Heywood & Mark Rolston is impressively vile as Bogs Diamond. Then there's Roger Deakins' lush cinematography as the camera gracefully glides in and out of the prison offering almost ethereal hope to our characters (yes, they are ours). The music pings in conjunction with the emotional flow of the movie too. Thomas Newman's score is mostly piano based, dovetailing neatly with Andy's state of mind, while the excellently selected soundtrack ranges from the likes of Hank Williams to the gorgeous Le Nozze di Figaro by Mozart. If you love Shawshank then it's a love that lasts a lifetime. Every viewing brings the same array of emotions - anger - revilement - happiness - sadness - inspiration and a warmth that can reduce the most hardened into misty eyed wonderment. Above all else, though, Shawshank offers hope - not just for characters in a movie - but for a better life and a better world for all of us. \n",
            " [[   98    50    16    57 20831    42     2   276    46    18     4   766\n",
            "      7     4    22   145    11 21733    19     6   425    15   161   202\n",
            "     76 16242     8   259    44    51    12    16    44     5    19  7107\n",
            "   1268  6202    89     8  2336    12 11209  3265  1201 10186    33     4\n",
            "    953  1052   190   216  1809  1344    58     4    22  1990   450  6146\n",
            "      5   261    12  1199   600    12 14961    56   602    11     4    22\n",
            "     18    94   344   722   766     4   360    17    36   135     9   479\n",
            "     18     4    22   417   258    35   311    15   219     4    22 16149\n",
            "      8   220  8477  8726    17    35  3308   682   251   356   142    15\n",
            "     47  6639    94   451  1864  5189 11078    94 13743    31   155     9\n",
            "     18   252   151     9    15    63   126   499     7     4 11209  8740\n",
            "     25   870    23     4    22  1999     8  5231   162   451     5   331\n",
            "     80   115   140   245    42  1888    15  8477  2668    45   869     4\n",
            "   4678     7    12    32    15  3292    49 12507     7     4    22    83\n",
            "   1360 86360     4  4035   114     5    35  1734  3178  1289    15   166\n",
            "      6  1835     7  1172   113    26    21   107  2297  5266    39   148\n",
            "     15  3122     4    22    19     6  1797   246    54   105    26    14\n",
            "   8632  1309     5    38 34026  2566    12  3372    72    17 33329     8\n",
            "     81   180     6   406   453   198  1951    11   440  1862     5  1804\n",
            "      4  3178  1251     9   849    50    21    15  1421    17     6 12702\n",
            "      8     4  2073 12687     5  9199     7     4  1357  1237   263  2311\n",
            "  37999  1172   113    25     2    57   580     4   359    18   406  2025\n",
            "      9   115    53   887    74   315 20701     2     5   348     4   179\n",
            "   1307   354     7  5663   115   128     2  2831 21997   231    12   776\n",
            "     45     4 11352   155    11     4   182     8  2272     8  1798     5\n",
            "    767   148    11  1425   713 32404    99    78   345  2046     2     9\n",
            "      2  3673 73525    17  7413  9128   592 32105     9   483 12812    87\n",
            "     17     4 58789     7 11209 32214  2115     9  3541    17  7232  9590\n",
            "   8579  7114  1024 27080  1139    17 32773     2   951     2     9 10296\n",
            "   6031    17 19527  3643    95   225  2477     2  5467   627    17     4\n",
            "    370 12401 23549    11     5    46     7     4  1172  3991   220 12468\n",
            "    440     8   263   105   422    36    26 11295     4   228     2    11\n",
            "  20274    19     4   921  2973     7     4    20    99  2102 17612   603\n",
            "      9   669  3654   448     2  6009    19 12084  1110     7   330   137\n",
            "      4  6240  6546   816 10423    39     4  1232     7  4391  1701     8\n",
            "      4  1490  3458     2 10029 50315    34 23042    48    25   119 11209\n",
            "     95    45     6   119    15  6212     6  2646   175   829   961     4\n",
            "    172  7514     7  1438     2  2563     2     2     2  2652     2  3907\n",
            "      2  2867     5     6  4830    15    70 12028     4    91  7728    83\n",
            "   7594  3406 30335   752    32   334   151 11209  1580   440     2    24\n",
            "     43    18   105    11     6    20     2    21    18     6   128   113\n",
            "      5     6   128   182    18    32     7   178]] \n",
            " [0.9999274]\n",
            "Sentiment: Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sFOWhgAQ1s6B"
      },
      "source": [
        "We are able to achieve a score of \"highly positive\" on the review rated 10/10 on IMDb. Hence, our model is fairly accurate."
      ]
    }
  ]
}