{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"imdb_reviews.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1O9dTzHd9RmieCi53QR2Oz9wB4v5iTQJC","authorship_tag":"ABX9TyMjLgxy7UtDvPfVD61Gb6AG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Hb15qJcekavI","colab_type":"text"},"source":["#**Text Classification with the IMDb-Reviews Dataset from Keras**\n","@author: [vatsalya-gupta](https://github.com/vatsalya-gupta)"]},{"cell_type":"code","metadata":{"id":"z6EBWDDYEtSx","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","data = keras.datasets.imdb"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7kOEthtfnkBu","colab_type":"text"},"source":["We will do our analysis with the 85000 most frequent unique words."]},{"cell_type":"code","metadata":{"id":"iwPp1ourJAN5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"aec83e58-2965-4763-e9b7-e96a43c213bc","executionInfo":{"status":"ok","timestamp":1591366988661,"user_tz":-330,"elapsed":8128,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["(train_data, train_labels), (test_data, test_labels) = data.load_data(num_words = 85000)\n","print(train_data[0])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qu7omIyTlMq7","colab_type":"text"},"source":["Our training and testing data is in the form of an array of reviews, where each review is a list of integers and each integer represents a unique word. So we need to make it human readable. For this, we will be adding the following tags to the data, map the values to their respective keys and implement a function which converts the integers to the respective words."]},{"cell_type":"code","metadata":{"id":"oLm7rhdcJZPD","colab_type":"code","colab":{}},"source":["word_index = data.get_word_index()\n","word_index = {k: (v + 3) for k, v in word_index.items()}\n","word_index[\"<PAD>\"] = 0\n","word_index[\"<START>\"] = 1\n","word_index[\"<UNK>\"] = 2    # unknown\n","word_index[\"<UNUSED>\"] = 3\n","\n","reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9Ck0V79ledx","colab_type":"code","colab":{}},"source":["def decode_review(text):\n","\treturn \" \".join([reverse_word_index.get(i, \"?\") for i in text])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpL5VpjHL02l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"9a8b38ed-92b5-44d5-9930-985363f85f11","executionInfo":{"status":"ok","timestamp":1591366988671,"user_tz":-330,"elapsed":8105,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["print(decode_review(train_data[0]))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AaChBIYGM3jI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ae0dfe1d-d455-4a58-d293-737eea17a1ed","executionInfo":{"status":"ok","timestamp":1591366988674,"user_tz":-330,"elapsed":8101,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["print(len(train_data[0]), len(test_data[0]))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["218 68\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DuIMtysMndkN","colab_type":"text"},"source":["In the following block of code, we will be finding the length of the longest review in our dataset."]},{"cell_type":"code","metadata":{"id":"Q2fFYnOfNGQr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2dc10224-26c3-4737-c757-3aa58880bd47","executionInfo":{"status":"ok","timestamp":1591366988679,"user_tz":-330,"elapsed":8098,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["longest_train = max(len(l) for l in train_data)\n","longest_test = max(len(l) for l in test_data)\n","\n","max_words = max(longest_train, longest_test)\n","print(max_words)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["2494\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6zlusI0RoQ5a","colab_type":"text"},"source":["Even though the longest review is 2494 words long, we can safely limit the length of our reviews to 250 words as most of them are well below that. For the ones with length less than 250 words, we need to add padding to their end, as our model requires the review length to be 250 words."]},{"cell_type":"code","metadata":{"id":"Mna5HgcdQLgj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3148a363-34ef-4c18-86da-2ad41e161cb4","executionInfo":{"status":"ok","timestamp":1591366990387,"user_tz":-330,"elapsed":9799,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["train_data = keras.preprocessing.sequence.pad_sequences(train_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 250)\n","test_data = keras.preprocessing.sequence.pad_sequences(test_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 250)\n","\n","print(len(train_data[0]), len(test_data[0]))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["250 250\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ucvkCeKrqiJx","colab_type":"text"},"source":["We are using a Sequential model. An Embedding layer attempts to determine the meaning of each word in the sentence by mapping each word to a position in vector space (helps in grouping words like \"fantastic\" and \"awesome\"). The GlobalAveragePooling1D layer scales down our data's dimensions to make it easier computationally. The last two layers in our network are dense fully connected layers. The output layer is one neuron that uses the sigmoid function to get a value between 0 and 1 which will represent the likelihood of the review being negative or positive respectively."]},{"cell_type":"code","metadata":{"id":"NL8OsQrRQ94w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"0c946076-5d2b-41a9-be28-cfb695857bfe","executionInfo":{"status":"ok","timestamp":1591366991065,"user_tz":-330,"elapsed":10469,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["model = keras.Sequential()\n","model.add(keras.layers.Embedding(85000, 16))\n","model.add(keras.layers.GlobalAveragePooling1D())\n","model.add(keras.layers.Dense(16, activation = \"relu\"))\n","model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n","\n","model.summary()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 16)          1360000   \n","_________________________________________________________________\n","global_average_pooling1d (Gl (None, 16)                0         \n","_________________________________________________________________\n","dense (Dense)                (None, 16)                272       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 17        \n","=================================================================\n","Total params: 1,360,289\n","Trainable params: 1,360,289\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8CZdpVPYsRTH","colab_type":"text"},"source":["Compiling the data using the following parameters. We are using loss as \"binary_crossentropy\", as the expected output of our model is either 0 or 1, that is negative or positive."]},{"cell_type":"code","metadata":{"id":"1HLinov6UMcA","colab_type":"code","colab":{}},"source":["model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3sxQl_JqDr3","colab_type":"text"},"source":["Here we split the training data into training and validation sets."]},{"cell_type":"code","metadata":{"id":"h4zC21q9SbN8","colab_type":"code","colab":{}},"source":["X_val = train_data[:10000]\n","X_train = train_data[10000:]\n","\n","y_val = train_labels[:10000]\n","y_train = train_labels[10000:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFZ8IzeYs7m0","colab_type":"text"},"source":["The training data is fit onto the model and the results are evaluated."]},{"cell_type":"code","metadata":{"id":"wrvl9V3aSx1o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"316f5ccb-65d4-4b37-fc70-807e73fcbc1a","executionInfo":{"status":"ok","timestamp":1591367029615,"user_tz":-330,"elapsed":48979,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["fitModel = model.fit(X_train, y_train, epochs = 40, batch_size = 512, validation_data = (X_val, y_val), verbose = 1)\n","results = model.evaluate(test_data, test_labels)\n","\n","print(results)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/40\n","30/30 [==============================] - 1s 37ms/step - loss: 0.6917 - accuracy: 0.5568 - val_loss: 0.6892 - val_accuracy: 0.7054\n","Epoch 2/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.6845 - accuracy: 0.7393 - val_loss: 0.6795 - val_accuracy: 0.7442\n","Epoch 3/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.6695 - accuracy: 0.7751 - val_loss: 0.6612 - val_accuracy: 0.7383\n","Epoch 4/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.6430 - accuracy: 0.7712 - val_loss: 0.6324 - val_accuracy: 0.7866\n","Epoch 5/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.6046 - accuracy: 0.8037 - val_loss: 0.5947 - val_accuracy: 0.8019\n","Epoch 6/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.5571 - accuracy: 0.8281 - val_loss: 0.5507 - val_accuracy: 0.8131\n","Epoch 7/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.5043 - accuracy: 0.8521 - val_loss: 0.5052 - val_accuracy: 0.8231\n","Epoch 8/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.4510 - accuracy: 0.8673 - val_loss: 0.4627 - val_accuracy: 0.8388\n","Epoch 9/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.4015 - accuracy: 0.8859 - val_loss: 0.4251 - val_accuracy: 0.8512\n","Epoch 10/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.3579 - accuracy: 0.8983 - val_loss: 0.3945 - val_accuracy: 0.8562\n","Epoch 11/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.3206 - accuracy: 0.9055 - val_loss: 0.3696 - val_accuracy: 0.8653\n","Epoch 12/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.2888 - accuracy: 0.9155 - val_loss: 0.3496 - val_accuracy: 0.8711\n","Epoch 13/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.2617 - accuracy: 0.9223 - val_loss: 0.3348 - val_accuracy: 0.8716\n","Epoch 14/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.2379 - accuracy: 0.9305 - val_loss: 0.3215 - val_accuracy: 0.8761\n","Epoch 15/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.2175 - accuracy: 0.9360 - val_loss: 0.3105 - val_accuracy: 0.8800\n","Epoch 16/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.1992 - accuracy: 0.9425 - val_loss: 0.3021 - val_accuracy: 0.8822\n","Epoch 17/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.1830 - accuracy: 0.9477 - val_loss: 0.2954 - val_accuracy: 0.8829\n","Epoch 18/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.1682 - accuracy: 0.9535 - val_loss: 0.2899 - val_accuracy: 0.8847\n","Epoch 19/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.1552 - accuracy: 0.9591 - val_loss: 0.2856 - val_accuracy: 0.8862\n","Epoch 20/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.1432 - accuracy: 0.9632 - val_loss: 0.2824 - val_accuracy: 0.8862\n","Epoch 21/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.1323 - accuracy: 0.9668 - val_loss: 0.2797 - val_accuracy: 0.8861\n","Epoch 22/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.1226 - accuracy: 0.9703 - val_loss: 0.2797 - val_accuracy: 0.8874\n","Epoch 23/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.1136 - accuracy: 0.9739 - val_loss: 0.2768 - val_accuracy: 0.8873\n","Epoch 24/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.1050 - accuracy: 0.9769 - val_loss: 0.2776 - val_accuracy: 0.8868\n","Epoch 25/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.0978 - accuracy: 0.9795 - val_loss: 0.2757 - val_accuracy: 0.8877\n","Epoch 26/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.0909 - accuracy: 0.9819 - val_loss: 0.2758 - val_accuracy: 0.8883\n","Epoch 27/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.0845 - accuracy: 0.9835 - val_loss: 0.2777 - val_accuracy: 0.8884\n","Epoch 28/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.0788 - accuracy: 0.9850 - val_loss: 0.2771 - val_accuracy: 0.8890\n","Epoch 29/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.0733 - accuracy: 0.9864 - val_loss: 0.2787 - val_accuracy: 0.8891\n","Epoch 30/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.0686 - accuracy: 0.9874 - val_loss: 0.2796 - val_accuracy: 0.8887\n","Epoch 31/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.0640 - accuracy: 0.9888 - val_loss: 0.2815 - val_accuracy: 0.8887\n","Epoch 32/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.0600 - accuracy: 0.9899 - val_loss: 0.2833 - val_accuracy: 0.8884\n","Epoch 33/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.0561 - accuracy: 0.9910 - val_loss: 0.2862 - val_accuracy: 0.8884\n","Epoch 34/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.0524 - accuracy: 0.9919 - val_loss: 0.2877 - val_accuracy: 0.8884\n","Epoch 35/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.0490 - accuracy: 0.9923 - val_loss: 0.2897 - val_accuracy: 0.8881\n","Epoch 36/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.0458 - accuracy: 0.9931 - val_loss: 0.2915 - val_accuracy: 0.8885\n","Epoch 37/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.0431 - accuracy: 0.9942 - val_loss: 0.2959 - val_accuracy: 0.8876\n","Epoch 38/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.0406 - accuracy: 0.9942 - val_loss: 0.2962 - val_accuracy: 0.8884\n","Epoch 39/40\n","30/30 [==============================] - 1s 28ms/step - loss: 0.0380 - accuracy: 0.9951 - val_loss: 0.3036 - val_accuracy: 0.8850\n","Epoch 40/40\n","30/30 [==============================] - 1s 29ms/step - loss: 0.0359 - accuracy: 0.9949 - val_loss: 0.3047 - val_accuracy: 0.8857\n","782/782 [==============================] - 2s 3ms/step - loss: 0.3366 - accuracy: 0.8722\n","[0.33657485246658325, 0.8722000122070312]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zXGei7y3pU_2","colab_type":"text"},"source":["Sample prediction from testing data."]},{"cell_type":"code","metadata":{"id":"NXDGSeVGYod8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"2258edde-897f-45dc-a5c0-0b4f2f3d66fe","executionInfo":{"status":"ok","timestamp":1591367029620,"user_tz":-330,"elapsed":48957,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["test_review = test_data[0]\n","predict = model.predict([test_review])\n","print(\"Review:\\n\", decode_review(test_review))\n","print(\"Prediction:\", predict[0])\n","print(\"Actual:\", test_labels[0])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Review:\n"," <START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n","Prediction: [1.5715963e-10]\n","Actual: 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QloUm6_ppYQf","colab_type":"text"},"source":["Saving the model so that we don't have to train it again."]},{"cell_type":"code","metadata":{"id":"tSR71k3jbKji","colab_type":"code","colab":{}},"source":["model.save(\"imdb_model.h5\")    # any name ending with .h5\n","# model = keras.models.load_model(\"imdb_model.h5\")    # loading the model, use this in any other project for testing"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLkyZ73npn-A","colab_type":"text"},"source":["Function to encode a text based review into a list of integers."]},{"cell_type":"code","metadata":{"id":"IShn516EiWP-","colab_type":"code","colab":{}},"source":["def review_encode(s):\n","\tencoded = [1]\n","\n","\tfor word in s:\n","\t\tif word.lower() in word_index:\n","\t\t\tencoded.append(word_index[word.lower()])\n","\t\telse:\n","\t\t\tencoded.append(2)\n","\n","\treturn encoded"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xoldbSdrpvuR","colab_type":"text"},"source":["Evaluating our model on an [external review](https://www.imdb.com/review/rw2284594)."]},{"cell_type":"code","metadata":{"id":"FxVejEqpiLw0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":428},"outputId":"26ddf5b5-5f03-4af7-86e8-548800f8e2c8","executionInfo":{"status":"ok","timestamp":1591367029629,"user_tz":-330,"elapsed":48903,"user":{"displayName":"Vatsalya Gupta","photoUrl":"https://lh5.googleusercontent.com/-yOqr7dq03fc/AAAAAAAAAAI/AAAAAAAAJPA/vo0lIwO50tw/s64/photo.jpg","userId":"01224052808679671060"}}},"source":["with open(\"sample_data/test.txt\", encoding = \"utf-8\") as f:\n","\tfor line in f.readlines():\n","\t\tnline = line.replace(\",\", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace(\"\\\"\",\"\").strip().split(\" \")\n","\t\tencode = review_encode(nline)\n","\t\tencode = keras.preprocessing.sequence.pad_sequences([encode], value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 250)    # make the data 250 words long\n","\t\tpredict = model.predict(encode)\n","\t\tprint(line, \"\\n\", encode, \"\\n\", predict[0])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["The Shawshank Redemption is written and directed by Frank Darabont. It is an adaptation of the Stephen King novella Rita Hayworth and Shawshank Redemption. Starring Tim Robbins and Morgan Freeman, the film portrays the story of Andy Dufresne (Robbins), a banker who is sentenced to two life sentences at Shawshank State Prison for apparently murdering his wife and her lover. Andy finds it tough going but finds solace in the friendship he forms with fellow inmate Ellis \"Red\" Redding (Freeman). While things start to pick up when the warden finds Andy a prison job more befitting his talents as a banker. However, the arrival of another inmate is going to vastly change things for all of them. There was no fanfare or bunting put out for the release of the film back in 94, with a title that didn't give much inkling to anyone about what it was about, and with Columbia Pictures unsure how to market it, Shawshank Redemption barely registered at the box office. However, come Academy Award time the film received several nominations, and although it won none, it stirred up interest in the film for its home entertainment release. The rest, as they say, is history. For the film finally found an audience that saw the film propelled to almost mythical proportions as an endearing modern day classic. Something that has delighted its fans, whilst simultaneously baffling its detractors. One thing is for sure, though, is that which ever side of the Shawshank fence you sit on, the film continues to gather new fans and simply will never go away or loose that mythical status. It's possibly the simplicity of it all that sends some haters of the film into cinematic spasms. The implausible plot and an apparent sentimental edge that makes a nonsense of prison life, are but two chief complaints from those that dislike the film with a passion. Yet when characters are this richly drawn, and so movingly performed, it strikes me as churlish to do down a human drama that's dealing in hope, friendship and faith. The sentimental aspect is indeed there, but that acts as a counterpoint to the suffering, degradation and shattering of the soul involving our protagonist. Cosy prison life you say? No chance. The need for human connection is never more needed than during incarceration, surely? And given the quite terrific performances of Robbins (never better) & Freeman (sublimely making it easy), it's the easiest thing in the world to warm to Andy and Red. Those in support aren't faring too bad either. Bob Gunton is coiled spring smarm as Warden Norton, James Whitmore is heart achingly great as the \"Birdman Of Shawshank,\" Clancy Brown is menacing as antagonist Capt. Byron Hadley, William Sadler amusing as Heywood & Mark Rolston is impressively vile as Bogs Diamond. Then there's Roger Deakins' lush cinematography as the camera gracefully glides in and out of the prison offering almost ethereal hope to our characters (yes, they are ours). The music pings in conjunction with the emotional flow of the movie too. Thomas Newman's score is mostly piano based, dovetailing neatly with Andy's state of mind, while the excellently selected soundtrack ranges from the likes of Hank Williams to the gorgeous Le Nozze di Figaro by Mozart. If you love Shawshank then it's a love that lasts a lifetime. Every viewing brings the same array of emotions - anger - revilement - happiness - sadness - inspiration and a warmth that can reduce the most hardened into misty eyed wonderment. Above all else, though, Shawshank offers hope - not just for characters in a movie - but for a better life and a better world for all of us. \n"," [[  263  2311 37999  1172   113    25     2    57   580     4   359    18\n","    406  2025     9   115    53   887    74   315 20701     2     5   348\n","      4   179  1307   354     7  5663   115   128     2  2831 21997   231\n","     12   776    45     4 11352   155    11     4   182     8  2272     8\n","   1798     5   767   148    11  1425   713 32404    99    78   345  2046\n","      2     9     2  3673 73525    17  7413  9128   592 32105     9   483\n","  12812    87    17     4 58789     7 11209 32214  2115     9  3541    17\n","   7232  9590  8579  7114  1024 27080  1139    17 32773     2   951     2\n","      9 10296  6031    17 19527  3643    95   225  2477     2  5467   627\n","     17     4   370 12401 23549    11     5    46     7     4  1172  3991\n","    220 12468   440     8   263   105   422    36    26 11295     4   228\n","      2    11 20274    19     4   921  2973     7     4    20    99  2102\n","  17612   603     9   669  3654   448     2  6009    19 12084  1110     7\n","    330   137     4  6240  6546   816 10423    39     4  1232     7  4391\n","   1701     8     4  1490  3458     2 10029 50315    34 23042    48    25\n","    119 11209    95    45     6   119    15  6212     6  2646   175   829\n","    961     4   172  7514     7  1438     2  2563     2     2     2  2652\n","      2  3907     2  2867     5     6  4830    15    70 12028     4    91\n","   7728    83  7594  3406 30335   752    32   334   151 11209  1580   440\n","      2    24    43    18   105    11     6    20     2    21    18     6\n","    128   113     5     6   128   182    18    32     7   178]] \n"," [0.9927902]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sFOWhgAQ1s6B","colab_type":"text"},"source":["We are able to achieve a score of \"highly positive\" on the review rated 10/10 on IMDb. Hence, our model is fairly accurate."]}]}
