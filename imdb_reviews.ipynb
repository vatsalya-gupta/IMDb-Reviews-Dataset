{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_reviews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb15qJcekavI",
        "colab_type": "text"
      },
      "source": [
        "#**Text Classification with the IMDb-Reviews Dataset from Keras**\n",
        "@author: [vatsalya-gupta](https://github.com/vatsalya-gupta)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6EBWDDYEtSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "imdb = keras.datasets.imdb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kOEthtfnkBu",
        "colab_type": "text"
      },
      "source": [
        "We will do our analysis with the 10000 most frequent unique words in our dataset. Here, we are making the train-test split, 80 % and 20 % respectively. Afterwards, we will split train_data into training and validation sets, making the final train-test-validate split be 60-20-20 %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwPp1ourJAN5",
        "colab_type": "code",
        "outputId": "07868d25-3e7c-48a2-cf22-abbe9fb67636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "(train_data, train_targets), (test_data, test_targets) = imdb.load_data(num_words = 10000)\n",
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "targets = np.concatenate((train_targets, test_targets), axis=0)\n",
        "\n",
        "test_data = data[:10000]\n",
        "test_labels = targets[:10000]\n",
        "train_data = data[10000:]\n",
        "train_labels = targets[10000:]\n",
        "\n",
        "print(train_data[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 13, 104, 14, 9, 31, 7, 4, 4343, 7, 4, 3776, 3394, 2, 495, 103, 141, 87, 2048, 17, 76, 2, 44, 164, 525, 13, 197, 14, 16, 338, 4, 177, 16, 6118, 5253, 2, 2, 2, 21, 61, 1126, 2, 16, 15, 36, 4621, 19, 4, 2, 157, 5, 605, 46, 49, 7, 4, 297, 8, 276, 11, 4, 621, 837, 844, 10, 10, 25, 43, 92, 81, 2282, 5, 95, 947, 19, 4, 297, 806, 21, 15, 9, 43, 355, 13, 119, 49, 3636, 6951, 43, 40, 4, 375, 415, 21, 2, 92, 947, 19, 4, 2282, 1771, 14, 5, 106, 2, 1151, 48, 25, 181, 8, 67, 6, 530, 9089, 1253, 7, 4, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu7omIyTlMq7",
        "colab_type": "text"
      },
      "source": [
        "Our training and testing data is in the form of an array of reviews, where each review is a list of integers and each integer represents a unique word. So we need to make it human readable. For this, we will be adding the following tags to the data, map the values to their respective keys and implement a function which converts the integers to the respective words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLm7rhdcJZPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()\n",
        "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2    # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9Ck0V79ledx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_review(text):\n",
        "\treturn \" \".join([reverse_word_index.get(i, \"?\") for i in text])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpL5VpjHL02l",
        "colab_type": "code",
        "outputId": "3e972a63-c0cb-4f24-dc56-03324b876b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(decode_review(train_data[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> i think this is one of the weakest of the kenneth branagh <UNK> works after such great efforts as much <UNK> about nothing etc i thought this was poor the cast was weaker alicia <UNK> <UNK> <UNK> but my biggest <UNK> was that they messed with the <UNK> work and cut out some of the play to put in the musical dance sequences br br you just don't do shakespeare and then mess with the play sorry but that is just wrong i love some cole porter just like the next person but <UNK> don't mess with the shakespeare skip this and watch <UNK> books if you want to see a brilliant shakespearean adaptation of the <UNK>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaChBIYGM3jI",
        "colab_type": "code",
        "outputId": "3f308bd2-c680-49c6-d09a-c7f9ef4e97c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(train_data[0]), len(test_data[0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118 218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuIMtysMndkN",
        "colab_type": "text"
      },
      "source": [
        "In the following block of code, we will be finding the length of the longest review in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2fFYnOfNGQr",
        "colab_type": "code",
        "outputId": "6d1e49c4-032e-43cf-faf1-3314f13d477e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "longest_train = max(len(l) for l in train_data)\n",
        "longest_test = max(len(l) for l in test_data)\n",
        "\n",
        "max_words = max(longest_train, longest_test)\n",
        "print(max_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zlusI0RoQ5a",
        "colab_type": "text"
      },
      "source": [
        "Even though the longest review is 2494 words long, we can safely limit the length of our reviews to 500 words as most of them are well below that. For the ones with length less than 500 words, we will add zero padding to their end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mna5HgcdQLgj",
        "colab_type": "code",
        "outputId": "5500eafc-998f-49fe-d1e1-58cf5802487b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 500)\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 500)\n",
        "\n",
        "print(len(train_data[0]), len(test_data[0]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvkCeKrqiJx",
        "colab_type": "text"
      },
      "source": [
        "We are using a Sequential model. An Embedding layer attempts to determine the meaning of each word in the sentence by mapping each word to a position in vector space (helps in grouping words like \"fantastic\" and \"awesome\"). The GlobalAveragePooling1D layer scales down our data's dimensions to make it easier computationally. A Dropout layer is added to decrease overfitting. The last two layers in our network are dense fully connected layers. The output layer is one neuron that uses the sigmoid function to get a value between 0 and 1 which will represent the likelihood of the review being negative or positive respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL8OsQrRQ94w",
        "colab_type": "code",
        "outputId": "b53b6bcd-15c6-43c6-94db-aea23c668a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(10000, 64))    # 10000 words as input\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          640000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 644,225\n",
            "Trainable params: 644,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CZdpVPYsRTH",
        "colab_type": "text"
      },
      "source": [
        "Compiling the data using the following parameters. We are using loss as \"binary_crossentropy\", as the expected output of our model is either 0 or 1, that is negative or positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLinov6UMcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFZ8IzeYs7m0",
        "colab_type": "text"
      },
      "source": [
        "Here we split the training data into training and validation sets, then the training data is fit onto the model and the results are evaluated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrvl9V3aSx1o",
        "colab_type": "code",
        "outputId": "550aeb20-8ed9-4ace-dbe1-92c731ba2e47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "fitModel = model.fit(train_data, train_labels, epochs = 8, batch_size = 256, validation_split = 0.25, verbose = 1)\n",
        "results = model.evaluate(test_data, test_labels)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "118/118 [==============================] - 3s 27ms/step - loss: 0.6789 - accuracy: 0.6016 - val_loss: 0.6242 - val_accuracy: 0.7426\n",
            "Epoch 2/8\n",
            "118/118 [==============================] - 3s 25ms/step - loss: 0.4919 - accuracy: 0.8073 - val_loss: 0.3667 - val_accuracy: 0.8664\n",
            "Epoch 3/8\n",
            "118/118 [==============================] - 3s 25ms/step - loss: 0.3201 - accuracy: 0.8797 - val_loss: 0.2951 - val_accuracy: 0.8830\n",
            "Epoch 4/8\n",
            "118/118 [==============================] - 3s 25ms/step - loss: 0.2632 - accuracy: 0.8999 - val_loss: 0.2672 - val_accuracy: 0.8951\n",
            "Epoch 5/8\n",
            "118/118 [==============================] - 3s 26ms/step - loss: 0.2285 - accuracy: 0.9156 - val_loss: 0.2568 - val_accuracy: 0.8991\n",
            "Epoch 6/8\n",
            "118/118 [==============================] - 3s 26ms/step - loss: 0.2082 - accuracy: 0.9217 - val_loss: 0.2553 - val_accuracy: 0.8998\n",
            "Epoch 7/8\n",
            "118/118 [==============================] - 3s 25ms/step - loss: 0.1909 - accuracy: 0.9307 - val_loss: 0.2545 - val_accuracy: 0.9001\n",
            "Epoch 8/8\n",
            "118/118 [==============================] - 3s 25ms/step - loss: 0.1761 - accuracy: 0.9356 - val_loss: 0.2562 - val_accuracy: 0.8979\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2717 - accuracy: 0.8972\n",
            "[0.27172037959098816, 0.8971999883651733]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGei7y3pU_2",
        "colab_type": "text"
      },
      "source": [
        "Sample prediction from testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXDGSeVGYod8",
        "colab_type": "code",
        "outputId": "d1c1f439-2420-4d02-d6fd-a39fe0e47d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "test_review = test_data[6]\n",
        "predict = model.predict(test_review)\n",
        "print(\"Review:\\n\", decode_review(test_review))\n",
        "print(\"Prediction:\", predict[6])\n",
        "print(\"Actual:\", test_labels[6])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review:\n",
            " <START> lavish production values and solid performances in this straightforward adaption of jane <UNK> satirical classic about the marriage game within and between the classes in <UNK> 18th century england northam and paltrow are a <UNK> mixture as friends who must pass through <UNK> and lies to discover that they love each other good humor is a <UNK> virtue which goes a long way towards explaining the <UNK> of the aged source material which has been toned down a bit in its harsh <UNK> i liked the look of the film and how shots were set up and i thought it didn't rely too much on <UNK> of head shots like most other films of the 80s and 90s do very good results <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "Prediction: [1.]\n",
            "Actual: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QloUm6_ppYQf",
        "colab_type": "text"
      },
      "source": [
        "Saving the model so that we don't have to train it again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSR71k3jbKji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"imdb_model.h5\")    # any name ending with .h5\n",
        "# model = keras.models.load_model(\"imdb_model.h5\")    # loading the model, use this in any other project for testing"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLkyZ73npn-A",
        "colab_type": "text"
      },
      "source": [
        "Function to encode a text based review into a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IShn516EiWP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def review_encode(s):\n",
        "\tencoded = [1]\n",
        "\n",
        "\tfor word in s:\n",
        "\t\tif word.lower() in word_index:\n",
        "\t\t\tencoded.append(word_index[word.lower()] if (word_index[word.lower()] < 10001) else 2)    # vocabulary size is 10000\n",
        "\t\telse:\n",
        "\t\t\tencoded.append(2)    # 2 means \"<UNK>\"\n",
        "\n",
        "\treturn encoded"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoldbSdrpvuR",
        "colab_type": "text"
      },
      "source": [
        "Evaluating our model on an [external review](https://www.imdb.com/review/rw2284594)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxVejEqpiLw0",
        "colab_type": "code",
        "outputId": "23348fc1-6976-4649-e2d8-e6cc55e76ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "with open(\"sample_data/test.txt\", encoding = \"utf-8\") as f:\n",
        "\tfor line in f.readlines():\n",
        "\t\tnline = line.replace(\",\", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace(\"\\\"\",\"\").strip().split(\" \")\n",
        "\t\tencode = review_encode(nline)\n",
        "\t\tencode = keras.preprocessing.sequence.pad_sequences([encode], value = word_index[\"<PAD>\"], padding = \"post\", maxlen = 500)    # make the data 500 words long\n",
        "\t\tpredict = model.predict(encode)\n",
        "\t\tprint(line, \"\\n\", encode, \"\\n\", predict[0])\n",
        "\t\tsentiment = \"Positive\" if (predict[0] > 0.5) else \"Negative\"\n",
        "\t\tprint(\"Sentiment:\", sentiment)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Shawshank Redemption is written and directed by Frank Darabont. It is an adaptation of the Stephen King novella Rita Hayworth and Shawshank Redemption. Starring Tim Robbins and Morgan Freeman, the film portrays the story of Andy Dufresne (Robbins), a banker who is sentenced to two life sentences at Shawshank State Prison for apparently murdering his wife and her lover. Andy finds it tough going but finds solace in the friendship he forms with fellow inmate Ellis \"Red\" Redding (Freeman). While things start to pick up when the warden finds Andy a prison job more befitting his talents as a banker. However, the arrival of another inmate is going to vastly change things for all of them. There was no fanfare or bunting put out for the release of the film back in 94, with a title that didn't give much inkling to anyone about what it was about, and with Columbia Pictures unsure how to market it, Shawshank Redemption barely registered at the box office. However, come Academy Award time the film received several nominations, and although it won none, it stirred up interest in the film for its home entertainment release. The rest, as they say, is history. For the film finally found an audience that saw the film propelled to almost mythical proportions as an endearing modern day classic. Something that has delighted its fans, whilst simultaneously baffling its detractors. One thing is for sure, though, is that which ever side of the Shawshank fence you sit on, the film continues to gather new fans and simply will never go away or loose that mythical status. It's possibly the simplicity of it all that sends some haters of the film into cinematic spasms. The implausible plot and an apparent sentimental edge that makes a nonsense of prison life, are but two chief complaints from those that dislike the film with a passion. Yet when characters are this richly drawn, and so movingly performed, it strikes me as churlish to do down a human drama that's dealing in hope, friendship and faith. The sentimental aspect is indeed there, but that acts as a counterpoint to the suffering, degradation and shattering of the soul involving our protagonist. Cosy prison life you say? No chance. The need for human connection is never more needed than during incarceration, surely? And given the quite terrific performances of Robbins (never better) & Freeman (sublimely making it easy), it's the easiest thing in the world to warm to Andy and Red. Those in support aren't faring too bad either. Bob Gunton is coiled spring smarm as Warden Norton, James Whitmore is heart achingly great as the \"Birdman Of Shawshank,\" Clancy Brown is menacing as antagonist Capt. Byron Hadley, William Sadler amusing as Heywood & Mark Rolston is impressively vile as Bogs Diamond. Then there's Roger Deakins' lush cinematography as the camera gracefully glides in and out of the prison offering almost ethereal hope to our characters (yes, they are ours). The music pings in conjunction with the emotional flow of the movie too. Thomas Newman's score is mostly piano based, dovetailing neatly with Andy's state of mind, while the excellently selected soundtrack ranges from the likes of Hank Williams to the gorgeous Le Nozze di Figaro by Mozart. If you love Shawshank then it's a love that lasts a lifetime. Every viewing brings the same array of emotions - anger - revilement - happiness - sadness - inspiration and a warmth that can reduce the most hardened into misty eyed wonderment. Above all else, though, Shawshank offers hope - not just for characters in a movie - but for a better life and a better world for all of us. \n",
            " [[  98   50   16   57    2   42    2  276   46   18    4  766    7    4\n",
            "    22  145   11    2   19    6  425   15  161  202   76    2    8  259\n",
            "    44   51   12   16   44    5   19 7107 1268 6202   89    8 2336   12\n",
            "     2 3265 1201    2   33    4  953 1052  190  216 1809 1344   58    4\n",
            "    22 1990  450 6146    5  261   12 1199  600   12    2   56  602   11\n",
            "     4   22   18   94  344  722  766    4  360   17   36  135    9  479\n",
            "    18    4   22  417  258   35  311   15  219    4   22    2    8  220\n",
            "  8477 8726   17   35 3308  682  251  356  142   15   47 6639   94  451\n",
            "  1864 5189    2   94    2   31  155    9   18  252  151    9   15   63\n",
            "   126  499    7    4    2 8740   25  870   23    4   22 1999    8 5231\n",
            "   162  451    5  331   80  115  140  245   42 1888   15 8477 2668   45\n",
            "   869    4 4678    7   12   32   15 3292   49    2    7    4   22   83\n",
            "  1360    2    4 4035  114    5   35 1734 3178 1289   15  166    6 1835\n",
            "     7 1172  113   26   21  107 2297 5266   39  148   15 3122    4   22\n",
            "    19    6 1797  246   54  105   26   14 8632 1309    5   38    2 2566\n",
            "    12 3372   72   17    2    8   81  180    6  406  453  198 1951   11\n",
            "   440 1862    5 1804    4 3178 1251    9  849   50   21   15 1421   17\n",
            "     6    2    8    4 2073    2    5 9199    7    4 1357 1237  263 2311\n",
            "     2 1172  113   25    2   57  580    4  359   18  406 2025    9  115\n",
            "    53  887   74  315    2    2    5  348    4  179 1307  354    7 5663\n",
            "   115  128    2 2831    2  231   12  776   45    4    2  155   11    4\n",
            "   182    8 2272    8 1798    5  767  148   11 1425  713    2   99   78\n",
            "   345 2046    2    9    2 3673    2   17 7413 9128  592    2    9  483\n",
            "     2   87   17    4    2    7    2    2 2115    9 3541   17 7232 9590\n",
            "  8579 7114 1024    2 1139   17    2    2  951    2    9    2 6031   17\n",
            "     2 3643   95  225 2477    2 5467  627   17    4  370    2    2   11\n",
            "     5   46    7    4 1172 3991  220    2  440    8  263  105  422   36\n",
            "    26    2    4  228    2   11    2   19    4  921 2973    7    4   20\n",
            "    99 2102    2  603    9  669 3654  448    2 6009   19    2 1110    7\n",
            "   330  137    4 6240 6546  816    2   39    4 1232    7 4391 1701    8\n",
            "     4 1490 3458    2    2    2   34    2   48   25  119    2   95   45\n",
            "     6  119   15 6212    6 2646  175  829  961    4  172 7514    7 1438\n",
            "     2 2563    2    2    2 2652    2 3907    2 2867    5    6 4830   15\n",
            "    70    2    4   91 7728   83 7594 3406    2  752   32  334  151    2\n",
            "  1580  440    2   24   43   18  105   11    6   20    2   21   18    6\n",
            "   128  113    5    6  128  182   18   32    7  178]] \n",
            " [0.99945503]\n",
            "Sentiment: Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFOWhgAQ1s6B",
        "colab_type": "text"
      },
      "source": [
        "We are able to achieve a score of \"highly positive\" on the review rated 10/10 on IMDb. Hence, our model is fairly accurate."
      ]
    }
  ]
}
